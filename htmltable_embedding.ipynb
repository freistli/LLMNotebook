{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from llama_index.indices.knowledge_graph.base import GPTKnowledgeGraphIndex\n",
    "from llama_index import (\n",
    "    GPTSimpleKeywordTableIndex,\n",
    "    GPTVectorStoreIndex,\n",
    "    LLMPredictor,\n",
    "    LangchainEmbedding,\n",
    "    OpenAIEmbedding,\n",
    "    PromptHelper,\n",
    "    ServiceContext,\n",
    "    SimpleWebPageReader,\n",
    "    TrafilaturaWebReader,\n",
    "    GPTKeywordTableIndex\n",
    ") \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms  import AzureOpenAI\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "_ = load_dotenv(find_dotenv('.env_x'),override=True)  \n",
    "\n",
    "index = None\n",
    "\n",
    " # max LLM token input size\n",
    "max_input_size = 500\n",
    " # set number of output tokens\n",
    "num_output = 100\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20 \n",
    "\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "\n",
    "if(os.environ['OPENAI_API_TYPE'] == 'azure'):\n",
    "            openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "            openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "            openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "            openai.api_key = os.environ['OPENAI_API_KEY']                    \n",
    "            llm = AzureOpenAI(deployment_name=\"newtextmodel\")            \n",
    "            llm_predictor = LLMPredictor(llm=llm)\n",
    "            #embedding model deployment name has to be \"text-embedding-ada-002\" on Azure OpenAI      \n",
    "            embedding_llm = LangchainEmbedding(OpenAIEmbeddings(chunk_size=512))\n",
    "            prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "            service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embedding_llm, prompt_helper=prompt_helper)\n",
    "else:\n",
    "            openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "            openai.api_base = \"https://api.openai.com/v1\"\n",
    "            openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "            openai.api_version = \"\"\n",
    "            llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "            embedding_llm = LangchainEmbedding(OpenAIEmbeddings(chunk_size=4096))\n",
    "            service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor,embed_model=embedding_llm) \n",
    " \n",
    "storagePath = './webstorage_SurfaceBookandPro'\n",
    "dataSource = [\"https://support.microsoft.com/en-us/surface/surface-pro-8-update-history-1080bf34-7e87-408c-8619-80571283526e\",\n",
    "              \"https://support.microsoft.com/en-us/surface/surface-book-3-update-history-935a7b6b-2f6d-dbf9-b3ba-0ea61e187b2d\"]\n",
    "\n",
    "try:\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=storagePath)\n",
    "    # load index\n",
    "    index = load_index_from_storage(storage_context = storage_context, service_context = service_context)\n",
    "     \n",
    "except:\n",
    "    print(\"No index found, creating a new one\")\n",
    "\n",
    "urls = dataSource\n",
    "\n",
    "if(index is None):    \n",
    "    documents = SimpleWebPageReader(html_to_text=False).load_data(urls)  \n",
    "    #documents = TrafilaturaWebReader().load_data(urls)\n",
    "    index = GPTVectorStoreIndex.from_documents(documents,service_context=service_context)\n",
    "    index.storage_context.persist(persist_dir=storagePath)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=5)\n",
    "deviceName = \"Surface System Aggregator - Firmware\"\n",
    "#deviceName = \"Surface UEFI - Firmware\"\n",
    "tableFormatPrompt = \"All HTML tables <table></table> in the file have two columns. The first column contains driver name and version info. The second column contains device name info. \"\n",
    "versionIinfoPrompt = tableFormatPrompt +\"It is very possible that more than one HTML table contains the same device name info. Please search all HTML tables and return all matched rows which contains the same \\\"\" + deviceName + \"\\\" as the second column content. \"\n",
    "#versionFormatPrompt = versionIinfoPrompt + \"If the response content from multiple rows, return the content of the driver name and the version info which version info has largest version number.\"\n",
    "versionFormatPrompt = versionIinfoPrompt + \"Start the response with all matched rows directly, use CSV format.\"\n",
    "result = query_engine.query(versionFormatPrompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
