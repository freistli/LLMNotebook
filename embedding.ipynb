{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06765691  0.06349587  0.04871308 ...  0.03807081  0.05996544\n",
      "  -0.0422287 ]\n",
      " [ 0.08643859  0.10276265  0.00539457 ...  0.07028491  0.0749312\n",
      "  -0.08417752]\n",
      " [ 0.04945167  0.02024709 -0.06004903 ... -0.02235334  0.00888847\n",
      "  -0.07535408]]\n",
      "(3, 384)\n"
     ]
    }
   ],
   "source": [
    "#hugging face embedding model\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\", \"Individually into a vector\"]\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') \n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#openai embedding model\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "_ = load_dotenv(dotenv_path = find_dotenv('.env_x'), override=True)\n",
    "\n",
    "openai.api_base = \"https://api.openai.com/v1\"\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "openai.api_version = \"\"\n",
    "\n",
    "embedding = openai.Embedding.create(\n",
    "    input=\"This is an example sentence\", model=\"text-embedding-ada-002\")[\"data\"][0][\"embedding\"]\n",
    "len(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transfered to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b6351dd3ab334c9ab1d3669aab6e680a\n",
      "azure\n",
      "\u001b[1mAzureOpenAI\u001b[0m\n",
      "Params: {'deployment_name': '', 'model_name': 'text-davinci-003', 'temperature': 0.7, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}, 'engine': 'chatgpt'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' doing?”. I would have never guessed that our teamwork would make our project so successful. The success of our project was due to everyone’s contribution and hard work.\\n\\nI have learned a lot from this project. The most important thing that I have learned is that teamwork is the key to success and that I should always work in a team in order to achieve my goals. It was a great experience working with such a great team and I am sure that we will work together again in the future.\\n\\nI would like to thank each and every member of our team for their contribution and hard work. Without their efforts, our project would not have been so successful. I am grateful to have had the opportunity to work with such a talented and dedicated team. Thank you all again for your hard work and dedication.\\n\\nSincerely,\\n\\n[Your Name]\\n\\n[Your Position]\\n\\n[Company Name]”\\n\\nSample 2: Appreciation Letter for Project Completion\\n\\n“Dear Team,\\n\\nI would like to express my heartfelt appreciation to each and every one of you for your hard work and dedication in completing our project on time. Your commitment and dedication to the project have been truly impressive.\\n\\nI am proud of the way we worked together as a team and delivered a high-quality project. Your professionalism and dedication have been'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Azure OpenAI model\n",
    "import openai\n",
    "from langchain.llms import AzureOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "_ = load_dotenv(find_dotenv(),override=True) # read local .env file\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "print(openai.api_key)\n",
    "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "print(openai.api_type)\n",
    "\n",
    "# Create an instance of Azure OpenAI\n",
    "# Replace the deployment name with your own\n",
    "llm = AzureOpenAI(engine=\"chatgpt\")\n",
    "\n",
    "print(llm)\n",
    "# Run the LLM\n",
    "llm(\"how are you\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 7 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 326 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article is discussing the author's experiences with writing and programming before college, including their early attempts at writing short stories and programming on an IBM 1401 using an early version of Fortran.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from llama_index import (\n",
    "    GPTVectorStoreIndex,\n",
    "    LLMPredictor,\n",
    "    ServiceContext\n",
    ") \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader\n",
    "import os\n",
    "\n",
    "#logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "#logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "_ = load_dotenv(find_dotenv('.env_x'),override=True)  \n",
    "openai.api_base = \"https://api.openai.com/v1\"\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "openai.api_version = \"\"\n",
    "index = None\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    " \n",
    "try:\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir='./storage')\n",
    "    # load index\n",
    "    index = load_index_from_storage(storage_context = storage_context, service_context = service_context)\n",
    "     \n",
    "except:\n",
    "    print(\"No index found, creating a new one\")\n",
    "\n",
    "if(index is None):    \n",
    "    documents = SimpleDirectoryReader('data').load_data()    \n",
    "    index = GPTVectorStoreIndex.from_documents(documents,service_context=service_context)\n",
    "    index.storage_context.persist()\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "result = query_engine.query(\"what is this article talking about?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article is discussing the author's experiences with writing and programming before college, including their early attempts at writing short stories and programming on an IBM 1401 using an early version of Fortran.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
